---
layout:     post
title:      Let me find your Spotify music
author:     Sungho Kim
<!-- tags: 		post template -->
subtitle:  	Using the DecisionTree Algorithm in Python
<!-- category:  project1 -->
---
![Description](/img/post/spotify/logo.png)

I love listening to music, however often, the hardest time comes, when I get tired of my playlist, and have to search for music that I want to hear. I am sure that you have experienced the same situation. In this post, I am going to build a machine learning model that can predict whether or not I will like a song by using classify Decision Tree algorithm.



I collected the dataset from Spotify’s API, and each song is either labeled “1” meaning I like it or “0” meaning that I don't like it. First I am going to import the libraries and load the dataset.

<pre style='color:#d1d1d1;background:#000000;'><span style='color:#e66170; font-weight:bold; '>import</span> numpy <span style='color:#e66170; font-weight:bold; '>as</span> np
<span style='color:#e66170; font-weight:bold; '>import</span> pandas <span style='color:#e66170; font-weight:bold; '>as</span> pd 
<span style='color:#e66170; font-weight:bold; '>from</span> matplotlib <span style='color:#e66170; font-weight:bold; '>import</span> pyplot <span style='color:#e66170; font-weight:bold; '>as</span> plt

<span style='color:#e66170; font-weight:bold; '>from</span> scipy <span style='color:#e66170; font-weight:bold; '>import</span> misc
<span style='color:#e66170; font-weight:bold; '>from</span> sklearn <span style='color:#e66170; font-weight:bold; '>import</span> tree
<span style='color:#e66170; font-weight:bold; '>from</span> sklearn<span style='color:#d2cd86; '>.</span>metrics <span style='color:#e66170; font-weight:bold; '>import</span> accuracy_score
<span style='color:#e66170; font-weight:bold; '>from</span> sklearn<span style='color:#d2cd86; '>.</span>model_selection <span style='color:#e66170; font-weight:bold; '>import</span> train_test_split
<span style='color:#e66170; font-weight:bold; '>from</span> sklearn<span style='color:#d2cd86; '>.</span>tree <span style='color:#e66170; font-weight:bold; '>import</span> DecisionTreeClassifier<span style='color:#d2cd86; '>,</span> export_graphviz

data <span style='color:#d2cd86; '>=</span> pd<span style='color:#d2cd86; '>.</span>read_csv<span style='color:#d2cd86; '>(</span><span style='color:#00c4c4; '>'spotify.csv'</span><span style='color:#d2cd86; '>)</span>
</pre>



**PREPARE DATA**

It is always important to check the dataset first and clean the data, before you run any code on them. I found out that “Unnamed: 0” column does not contain any relevant information on building a model, so I am going to drop it. In addition to that, drop all “NA”.


<pre style='color:#d1d1d1;background:#000000;'><span style='color:#e66170; font-weight:bold; '>print</span><span style='color:#d2cd86; '>(</span>data<span style='color:#d2cd86; '>.</span>describe<span style='color:#d2cd86; '>(</span><span style='color:#d2cd86; '>)</span><span style='color:#d2cd86; '>)</span>
half_count <span style='color:#d2cd86; '>=</span> <span style='color:#e66170; font-weight:bold; '>len</span><span style='color:#d2cd86; '>(</span>data<span style='color:#d2cd86; '>)</span> <span style='color:#00dddd; '>/</span> <span style='color:#00a800; '>2</span>
data <span style='color:#d2cd86; '>=</span> data<span style='color:#d2cd86; '>.</span>dropna<span style='color:#d2cd86; '>(</span>thresh<span style='color:#d2cd86; '>=</span>half_count<span style='color:#d2cd86; '>,</span>axis<span style='color:#d2cd86; '>=</span><span style='color:#00a800; '>1</span><span style='color:#d2cd86; '>)</span> 
data<span style='color:#d2cd86; '>.</span>drop<span style='color:#d2cd86; '>(</span><span style='color:#00c4c4; '>"Unnamed: 0"</span><span style='color:#d2cd86; '>,</span>axis<span style='color:#d2cd86; '>=</span><span style='color:#00a800; '>1</span><span style='color:#d2cd86; '>,</span>inplace<span style='color:#d2cd86; '>=</span>True<span style='color:#d2cd86; '>)</span>
</pre>

**FEATURE SELECTION**

Now it is time to select the features. I believe that the feature selection is important step, and there are huge tradeoffs for selecting right and removing certain features in a model. Running the following command will show what features that we have.


<pre style='color:#d1d1d1;background:#000000;'><span style='color:#e66170; font-weight:bold; '>print</span><span style='color:#d2cd86; '>(</span>data<span style='color:#d2cd86; '>.</span>columns<span style='color:#d2cd86; '>.</span>values<span style='color:#d2cd86; '>)</span>
</pre>

I am going to use the following features to train a model.

<pre style='color:#d1d1d1;background:#000000;'>features <span style='color:#d2cd86; '>=</span> <span style='color:#d2cd86; '>[</span><span style='color:#00c4c4; '>'acousticness'</span><span style='color:#d2cd86; '>,</span><span style='color:#00c4c4; '>'danceability'</span><span style='color:#d2cd86; '>,</span> <span style='color:#00c4c4; '>'duration_ms'</span><span style='color:#d2cd86; '>,</span> <span style='color:#00c4c4; '>'energy'</span><span style='color:#d2cd86; '>,</span> <span style='color:#00c4c4; '>'instrumentalness'</span><span style='color:#d2cd86; '>,</span> <span style='color:#00c4c4; '>'key'</span><span style='color:#d2cd86; '>,</span>
           <span style='color:#00c4c4; '>'liveness'</span><span style='color:#d2cd86; '>,</span> <span style='color:#00c4c4; '>'speechiness'</span><span style='color:#d2cd86; '>,</span> <span style='color:#00c4c4; '>'tempo'</span><span style='color:#d2cd86; '>,</span> <span style='color:#00c4c4; '>'time_signature'</span><span style='color:#d2cd86; '>,</span> <span style='color:#00c4c4; '>'valence'</span><span style='color:#d2cd86; '>]</span>
</pre>


**SPLIT DATA**
<pre style='color:#d1d1d1;background:#000000;'>Y_data <span style='color:#d2cd86; '>=</span> data<span style='color:#d2cd86; '>[</span><span style='color:#00c4c4; '>'target'</span><span style='color:#d2cd86; '>]</span>
X_data <span style='color:#d2cd86; '>=</span> data<span style='color:#d2cd86; '>[</span>features<span style='color:#d2cd86; '>]</span> 

X_train<span style='color:#d2cd86; '>,</span>X_test<span style='color:#d2cd86; '>,</span>Y_train<span style='color:#d2cd86; '>,</span>Y_test <span style='color:#d2cd86; '>=</span> train_test_split<span style='color:#d2cd86; '>(</span>X_data<span style='color:#d2cd86; '>,</span>Y_data<span style='color:#d2cd86; '>,</span>random_state<span style='color:#d2cd86; '>=</span><span style='color:#00a800; '>42</span><span style='color:#d2cd86; '>,</span>stratify<span style='color:#d2cd86; '>=</span>Y_data<span style='color:#d2cd86; '>)</span> <span style='color:#9999a9; '>#what does stratify do?</span>
<span style='color:#9999a9; '># print(Y_train.shape, Y_test.shape)</span>
</pre>

Now it is time to implement Decision Tree Classifier. 
 
<pre style='color:#d1d1d1;background:#000000;'>clf <span style='color:#d2cd86; '>=</span> tree<span style='color:#d2cd86; '>.</span>DecisionTreeClassifier<span style='color:#d2cd86; '>(</span>min_samples_leaf<span style='color:#d2cd86; '>=</span><span style='color:#00a800; '>8</span><span style='color:#d2cd86; '>,</span> random_state<span style='color:#d2cd86; '>=</span><span style='color:#00a800; '>42</span><span style='color:#d2cd86; '>)</span> 
dt <span style='color:#d2cd86; '>=</span> clf<span style='color:#d2cd86; '>.</span>fit<span style='color:#d2cd86; '>(</span>X_train<span style='color:#d2cd86; '>,</span> y_train<span style='color:#d2cd86; '>)</span> 

y_pred <span style='color:#d2cd86; '>=</span> clf<span style='color:#d2cd86; '>.</span>predict<span style='color:#d2cd86; '>(</span>X_test<span style='color:#d2cd86; '>)</span> 
<span style='color:#9999a9; '>#print(y_pred) </span>

score <span style='color:#d2cd86; '>=</span> accuracy_score<span style='color:#d2cd86; '>(</span>y_test<span style='color:#d2cd86; '>,</span> y_pred<span style='color:#d2cd86; '>)</span> <span style='color:#00dddd; '>*</span> <span style='color:#00a800; '>100</span>
<span style='color:#e66170; font-weight:bold; '>print</span><span style='color:#d2cd86; '>(</span><span style='color:#00c4c4; '>"Decision Tree Classifier Accuracy: {}%"</span><span style='color:#d2cd86; '>.</span>format<span style='color:#d2cd86; '>(</span>score<span style='color:#d2cd86; '>)</span><span style='color:#d2cd86; '>)</span>
</pre>
I got 75.495% accuracy in this model.


If you want to optimize the hyperparmeters, such as max_features, min_samples_split, and min_samples_leaf, you can use K-Fold (10-Fold) cross validation with hyperparameter optimization using RandomSearchCV. You also have another option (Grid Search ), but it takes longer than Random Search.


<pre style='color:#d1d1d1;background:#000000;'><span style='color:#e66170; font-weight:bold; '>from</span> sklearn<span style='color:#d2cd86; '>.</span>model_selection <span style='color:#e66170; font-weight:bold; '>import</span> RandomizedSearchCV
<span style='color:#e66170; font-weight:bold; '>from</span> scipy<span style='color:#d2cd86; '>.</span>stats <span style='color:#e66170; font-weight:bold; '>import</span> randint <span style='color:#e66170; font-weight:bold; '>as</span> sp_randint

dt_clf <span style='color:#d2cd86; '>=</span> DecisionTreeClassifier<span style='color:#d2cd86; '>(</span>random_state<span style='color:#d2cd86; '>=</span><span style='color:#00a800; '>42</span><span style='color:#d2cd86; '>)</span>
param_grid <span style='color:#d2cd86; '>=</span> <span style='color:#b060b0; '>{</span><span style='color:#00c4c4; '>'max_features'</span><span style='color:#d2cd86; '>:</span> <span style='color:#d2cd86; '>[</span><span style='color:#00c4c4; '>'auto'</span><span style='color:#d2cd86; '>,</span> <span style='color:#00c4c4; '>'sqrt'</span><span style='color:#d2cd86; '>,</span> <span style='color:#00c4c4; '>'log2'</span><span style='color:#d2cd86; '>]</span><span style='color:#d2cd86; '>,</span>
              <span style='color:#00c4c4; '>'min_samples_split'</span><span style='color:#d2cd86; '>:</span> sp_randint<span style='color:#d2cd86; '>(</span><span style='color:#00a800; '>2</span><span style='color:#d2cd86; '>,</span> <span style='color:#00a800; '>11</span><span style='color:#d2cd86; '>)</span><span style='color:#d2cd86; '>,</span> 
              <span style='color:#00c4c4; '>'min_samples_leaf'</span><span style='color:#d2cd86; '>:</span> sp_randint<span style='color:#d2cd86; '>(</span><span style='color:#00a800; '>1</span><span style='color:#d2cd86; '>,</span> <span style='color:#00a800; '>11</span><span style='color:#d2cd86; '>)</span><span style='color:#b060b0; '>}</span>

rand_dt <span style='color:#d2cd86; '>=</span> RandomizedSearchCV<span style='color:#d2cd86; '>(</span>dt_clf<span style='color:#d2cd86; '>,</span> param_grid<span style='color:#d2cd86; '>,</span> cv<span style='color:#d2cd86; '>=</span><span style='color:#00a800; '>10</span><span style='color:#d2cd86; '>,</span> scoring<span style='color:#d2cd86; '>=</span><span style='color:#00c4c4; '>"accuracy"</span><span style='color:#d2cd86; '>,</span> n_iter<span style='color:#d2cd86; '>=</span><span style='color:#00a800; '>100</span><span style='color:#d2cd86; '>,</span> random_state<span style='color:#d2cd86; '>=</span><span style='color:#00a800; '>42</span><span style='color:#d2cd86; '>)</span>
rand_dt<span style='color:#d2cd86; '>.</span>fit<span style='color:#d2cd86; '>(</span>X_train<span style='color:#d2cd86; '>,</span>Y_train<span style='color:#d2cd86; '>)</span>

<span style='color:#e66170; font-weight:bold; '>print</span><span style='color:#d2cd86; '>(</span>rand_dt<span style='color:#d2cd86; '>.</span>best_score_<span style='color:#d2cd86; '>)</span>
<span style='color:#e66170; font-weight:bold; '>print</span><span style='color:#d2cd86; '>(</span>rand_dt<span style='color:#d2cd86; '>.</span>best_params_<span style='color:#d2cd86; '>)</span>
<span style='color:#e66170; font-weight:bold; '>print</span><span style='color:#d2cd86; '>(</span>rand_dt<span style='color:#d2cd86; '>.</span>best_estimator_<span style='color:#d2cd86; '>)</span>
</pre>

I hope you enjoyed this post, and please let me know if you have any questions, or feedback. You can reach me on [Linkedin](https://www.linkedin.com/in/sunghok/) or email me at <a href="mailto:sunghokim@wustl.edu?Subject=Hello%20again" target="_top">sunghokim@wustl.edu</a>.