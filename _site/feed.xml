<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>SUNGHO KIM</title>
    <description>&quot;What I cannot create, I do not understand.&quot;   -Richard P. Feynman-</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Mon, 18 Sep 2017 16:37:10 -0400</pubDate>
    <lastBuildDate>Mon, 18 Sep 2017 16:37:10 -0400</lastBuildDate>
    <generator>Jekyll v3.5.2</generator>
    
      <item>
        <title>Breast Cancer Prediction</title>
        <description>

Machine Learning Algorithms can be applied in many different areas. Not just in Netflix’s movie or Spotify’s music recommender engines, but also in the areas where have big and complicated data. Healthcare Industry is also where big data exist and have many problems to solve. Big companies like IBM(Watson) and Google(DeepMind) have already started applying and developing and machine learning healthcare applications. Besides them, there are also many young healthcare startups that use machine learning, such as Ayasdi, Nervanasys, Sentient.ai, and Reasoning Systems.

Some currently on going Machine Learning Healthcare Applications by the companies are Diagnosis in Medical Imaging, Drug discover, Robotic Surgery, Treatment Queries and Suggestions so on.

In this post, I am going to use two of Machine Learning Classifiers (Random Forrest &amp;amp; SVM) to build a simple model that can predict whether the patient’s cancer is benign or malignant. And I am going to use the Breast Cancer Wisconsin dataset.(Diagnostic)

PREPARE DATA

Load the data first, and see what it looks like. As you can see it from the picture, the data type of target value that we want to predict “diagnosis” is not integer, but categorical values “Malignant” and “Benign”. I changed them into the numbers “1” and “0” respectively. After that, I dropped any column with more than 50% missing values.

data = pd.read_csv(&quot;breast_cancer.csv&quot;)
print(data.head())
print(data.info())
data['diagnosis']=data['diagnosis'].map({'M':1,'B':0})  
half_count = len(data) / 2
data = data.dropna(thresh=half_count,axis=1)




Before I started building a model, I was curious about the distribution of the target Malignant and Benign. So I used the following command to visualize them. If you don’t want to plot them and just see the numbers, you can just type print(data[‘diagnosis’].value_counts()).

sns.countplot(data['diagnosis'],label=&quot;Count&quot;)
plt.show()



FEATURE SELECTION

Before the feature selection, I decided to visualize the correlation between the target and features. It looks like most of the features are positively related with the target, but I decided to select the features that have correlation values with target(“diagnosis)” from 0.55 to 1.

data_corr = data.corr()
data_corr['diagnosis'].sort_values(ascending=False)
data_corr['diagnosis'].sort_values().plot(kind='bar',sort_columns=True)
plt.show()



If you want to include more features, it is up to you. After the feature selection, I ran a command that draws a heat map of the correlation between target and selected features.

fig=plt.figure(figsize=(12,18))
selected_features_corr = data[selected_features].corr()
sns.heatmap(selected_features_corr, cbar = True,  square = True, annot=True, fmt= '.2f',annot_kws={'size': 8},
           xticklabels= selected_features, yticklabels= selected_features, cmap= 'coolwarm')



SPLIT DATA
Y_data = data['diagnosis']
X_data = data[selected_features].iloc[:,1:]
#print(X_data.columns.values)
X_train,X_test,Y_train,Y_test = train_test_split(X_data,Y_data,random_state=0,stratify=Y_data) 
# print(Y_train.shape, Y_test.shape)


RANDOM FORREST &amp;amp; SVM

Both models performed really well given the size of the dataset. I got Random Forrest model with an accuracy score of 0.951 and SVM model with an accuracy score of 0.965.

It looks like overfitting occurred in Random Forrest model, because it performed really well on training set, but poorly on test set. I tried to solve overfitting by tuning the hyperparameters of the model.

clf=RandomForestClassifier(n_estimators=100,random_state = 42)
clf.fit(X_train,Y_train)
print('Random Forrest Accuracy on the Training subset:{:.3f}'.format(clf.score(X_train,Y_train)))
print('Random Forrest Accuracy on the Test subset:{:.3f}'.format(clf.score(X_test,Y_test)))

svc = svm.SVC(kernel='linear')
svc.fit(X_train, Y_train)    
print('SVM Accuracy on the Training subset:{:.3f}'.format(svc.score(X_train,Y_train)))
print('SVM Accuracy on the Test subset:{:.3f}'.format(svc.score(X_test,Y_test)))



Hyperparameter Optimization

I used RandomSearchCV to optimize the hyperparameters. Now I got Random Forrest model with an accuracy score of 0.965. The model improved by 0.014 after the hyperparmeter optimization.

rf_clf=RandomForestClassifier(random_state = 42)
param_grid = {&quot;max_depth&quot;: [3, None],
              &quot;max_features&quot;:  sp_randint(1, 8),
              &quot;min_samples_split&quot;: sp_randint(2, 11),
              &quot;min_samples_leaf&quot;: sp_randint(1, 11),
              &quot;bootstrap&quot;: [True, False],
              &quot;criterion&quot;: [&quot;gini&quot;, &quot;entropy&quot;]}
rand_rf = RandomizedSearchCV(rf_clf, param_distributions=param_grid, n_iter=100, random_state=42)
rand_rf.fit(X_train,Y_train)
print('Random Forrest Accuracy on the Training subset:{:.3f}'.format(rand_rf.score(X_train,Y_train)))
print('Random Forrest Accuracy on the Test subset:{:.3f}'.format(rand_rf.score(X_test,Y_test)))



I hope you enjoyed this post, and please let me know if you have any questions, or feedback. You can reach me on Linkedin or email me at sunghokim@wustl.edu.

</description>
        <pubDate>Tue, 12 Sep 2017 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2017/09/12/breast_cancer_prediction/</link>
        <guid isPermaLink="true">http://localhost:4000/2017/09/12/breast_cancer_prediction/</guid>
        
        
      </item>
    
      <item>
        <title>Let me find your Spotify music</title>
        <description>

I love listening to music, however often, the hardest time comes, when I get tired of my playlist, and have to search for music that I want to hear. I am sure that you have experienced the same situation. In this post, I am going to build a machine learning model that can predict whether or not I will like a song by using classify Decision Tree algorithm.

I collected the dataset from Spotify’s API, and each song is either labeled “1” meaning I like it or “0” meaning that I don’t like it. First I am going to import the libraries and load the dataset.

import numpy as np
import pandas as pd 
from matplotlib import pyplot as plt

from scipy import misc
from sklearn import tree
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier, export_graphviz

data = pd.read_csv('spotify.csv')


PREPARE DATA

It is always important to check the dataset first and clean the data, before you run any code on them. I found out that “Unnamed: 0” column does not contain any relevant information on building a model, so I am going to drop it. In addition to that, drop all “NA”.

print(data.describe())
half_count = len(data) / 2
data = data.dropna(thresh=half_count,axis=1) 
data.drop(&quot;Unnamed: 0&quot;,axis=1,inplace=True)


FEATURE SELECTION

Now it is time to select the features. I believe that the feature selection is important step, and there are huge tradeoffs for selecting right and removing certain features in a model. Running the following command will show what features that we have.

print(data.columns.values)


I am going to use the following features to train a model.

features = ['acousticness','danceability', 'duration_ms', 'energy', 'instrumentalness', 'key',
           'liveness', 'speechiness', 'tempo', 'time_signature', 'valence']


SPLIT DATA
Y_data = data['target']
X_data = data[features] 

X_train,X_test,Y_train,Y_test = train_test_split(X_data,Y_data,random_state=42,stratify=Y_data) #what does stratify do?
# print(Y_train.shape, Y_test.shape)


Now it is time to implement Decision Tree Classifier.

clf = tree.DecisionTreeClassifier(min_samples_leaf=8, random_state=42) 
dt = clf.fit(X_train, y_train) 

y_pred = clf.predict(X_test) 
#print(y_pred) 

score = accuracy_score(y_test, y_pred) * 100
print(&quot;Decision Tree Classifier Accuracy: {}%&quot;.format(score))

I got 75.495% accuracy in this model.

If you want to optimize the hyperparmeters, such as max_features, min_samples_split, and min_samples_leaf, you can use K-Fold (10-Fold) cross validation with hyperparameter optimization using RandomSearchCV. You also have another option (Grid Search ), but it takes longer than Random Search.

from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint as sp_randint

dt_clf = DecisionTreeClassifier(random_state=42)
param_grid = {'max_features': ['auto', 'sqrt', 'log2'],
              'min_samples_split': sp_randint(2, 11), 
              'min_samples_leaf': sp_randint(1, 11)}

rand_dt = RandomizedSearchCV(dt_clf, param_grid, cv=10, scoring=&quot;accuracy&quot;, n_iter=100, random_state=42)
rand_dt.fit(X_train,Y_train)

print(rand_dt.best_score_)
print(rand_dt.best_params_)
print(rand_dt.best_estimator_)


I hope you enjoyed this post, and please let me know if you have any questions, or feedback. You can reach me on Linkedin or email me at sunghokim@wustl.edu.
</description>
        <pubDate>Tue, 05 Sep 2017 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2017/09/05/spotify/</link>
        <guid isPermaLink="true">http://localhost:4000/2017/09/05/spotify/</guid>
        
        
      </item>
    
      <item>
        <title>Who should FC Barcelona buy ?</title>
        <description>

Over the summer, there was a record-breaking deal in the soccer transfer market. Paris Saint-Germain triggered Neymar ’s release clause and paid €222 million to FC Barcelona. Last year, Paul Pogba’s deal from Juventus to Manchester United was a record-breaking, and the amount was €222. The transfer values just have increased exponentially.

In this post, I am not going to talk about if the value of Neymar is worth it or not, but how FC Barcelona should invest the money from Neymar’s fee into finding his successor. I am going to use the K-nearest neighbors(KNN) algorithm to find out which soccer players who have capabilities to replace “Neymar” based on the player’s stats from last season (2016-2017 season). I did not include Cristiano Ronaldo or other Real Madrid players in the dataset, because there is a huge rivalry between Real Madrid and FC Barcelona that players from Real Madrid would move to FC Barcelona.

First, normalize the dataset, then find the Euclidian distance between Neymar(target) and the other players. Since we are looking for a player that is most similar to Neymar, we should find the second most similar to Neymar. The distance between Neymar to Neymar is 0.

import pandas as pd
import math

soccer = pd.read_csv(&quot;2016.csv&quot;)
selected_player = soccer[soccer[&quot;player&quot;] == &quot;Neymar&quot;].iloc[0]

soccer_numeric = soccer[distance_columns]
soccer_normalized = (soccer_numeric - soccer_numeric.mean()) / soccer_numeric.std()

from scipy.spatial import distance
neymar_normalized = soccer_normalized[soccer[&quot;player&quot;] == &quot;Neymar&quot;]
euclidean_distances = soccer_normalized.apply(lambda row: distance.euclidean(row, neymar_normalized), axis=1)

distance_frame = pd.DataFrame(data={&quot;dist&quot;: euclidean_distances, &quot;idx&quot;: euclidean_distances.index})
distance_frame = distance_frame.sort_values([&quot;dist&quot;], ascending=True)

second_smallest = distance_frame.iloc[1][&quot;idx&quot;]
most_similar_to_neymar = soccer.loc[int(second_smallest)][&quot;player&quot;]
print(most_similar_to_neymar)


As a result of the analysis, it seems like Philippe Coutinho is most similar to Neymar. Angel Di Maria and Paulo Dybala are the second, third similar to Neymar respectively. According to the media, FC Barcelona actually had tried to buy Philippe Coutinho and Angel Di Maria before the transfer window closed. However, none of those deals happened, and FC Barcelona ended up signing Ousmane Dembele to replace Neymar.

I hope you enjoyed this post, and please let me know if you have any questions, or feedback. You can reach me on Linkedin or email me at sunghokim@wustl.edu.
</description>
        <pubDate>Fri, 01 Sep 2017 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2017/09/01/neymar/</link>
        <guid isPermaLink="true">http://localhost:4000/2017/09/01/neymar/</guid>
        
        
      </item>
    
      <item>
        <title>Mapping NYC Motor Vehicle Collisions Data</title>
        <description>

Everyday when I walk around NYC, I see a lot of traffic and sometimes car accidents on the street. This made me wonder if there are certain spots where car accidents happen more frequently than others. If there are patterns, I thought it would be fun to visualize and show them on the map. To solve my curiosity, I organized the procedures as following.


  Find the dataset
  Clean the dataset for mapping
  Visualize the dataset on Google map.


Finding the dataset
As always, choosing the right dataset is the hardest problem in the data
analysis process. However, I was able to find the dataset called “NYPD Motor Vehicle Collisions” from NYC Open Data website. It provides open data for people to use and explore for free. It not only provides city government or transportation data, but also education, business and more. So if you are interested what is happening around the town, I highly recommend you to take a look at the data and explore.

Cleaning the dataset 
The dataset has more than 10 columns and one million rows. If I mapped all million locations, the map would look like a Yayoi Kusama’s art work rather than a map. So I decided to reduce my dataset and focus on accidents that occurred in the last 3 months.

First, drop all the rows that have “NA” Second, select the columns that are needed for mapping (“LATIUDE” &amp;amp; “LONGTITUDE”) Third, make new column called “coordinate” that represents the spot. Finally, make another column called “frequency” which represents the frequency of coordinate.

import gmplot 
import datetime
import pandas as pd
import matplotlib.pyplot as plt  


df = pd.read_csv(&quot;2017.csv&quot;)
df = df.dropna(axis=0)

df[&quot;LATITUDE&quot;] = df[&quot;LATITUDE&quot;].astype(float)
df[&quot;LONGITUDE&quot;] = df[&quot;LONGITUDE&quot;].astype(float)
df = df[[&quot;LATITUDE&quot;,&quot;LONGITUDE&quot;]]
df.columns =[&quot;lat&quot;,&quot;long&quot;]

df[&quot;coordinate&quot;] = list(zip(df.lat, df.long))
df['freq'] = df.groupby('coordinate')['coordinate'].transform('count')


3.Visaulization 
I chose the color of the scatter spot by its frequency in the dataset.
Yellow (1~3), Green (4~9), Orange(10~17), Red (18~)

mapping = df[[&quot;lat&quot;,&quot;long&quot;,&quot;freq&quot;]]

yellow = mapping[mapping[&quot;freq&quot;]&amp;lt;=3] 
green  = mapping[(mapping[&quot;freq&quot;]&amp;gt;3) &amp;amp; (mapping[&quot;freq&quot;]&amp;lt;10)]
orange = mapping[(mapping[&quot;freq&quot;]&amp;lt;18) &amp;amp; (mapping[&quot;freq&quot;]&amp;gt;=10)]
red    = mapping[mapping[&quot;freq&quot;]&amp;gt;17] 

gmap = gmplot.GoogleMapPlotter(&quot;Latitude&quot;,&quot;Longitude&quot;, 12)
gmap.scatter(yellow.lat,yellow.long, &quot;yellow&quot; , size=15, marker=False)  
gmap.scatter(green.lat,green.long, &quot;green&quot; , size=20, marker=False)  
gmap.scatter(orange.lat,orange.long, &quot;orange&quot; , size=25, marker=False)  
gmap.scatter(red.lat,red.long, &quot;red&quot; , size=25, marker=False)


I hope you enjoyed this post, and please let me know if you have any questions, or feedback. You can reach me on Linkedin or email me at sunghokim@wustl.edu.
</description>
        <pubDate>Tue, 15 Aug 2017 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2017/08/15/nyc_taxi/</link>
        <guid isPermaLink="true">http://localhost:4000/2017/08/15/nyc_taxi/</guid>
        
        
      </item>
    
      <item>
        <title>Exploring Chipotle Order Data</title>
        <description>This data is based on about 1,800 Grubhub orders from July to December 2012. The data was collected from two Chipotle restaurants in Wahsington D.C &amp;amp; East Lansing. It seemed interesting for me to explore the dataset by asking questions like what items people order the most, what is the average order price, etc. I am going to use python to analyze the data.

First, Load the tsv file using Pandas and print(df.head(10)) to see the format of data and first 10 items on the list. I recommend you to do this on every data set, before you execute any commands on the data.

As you can see from table, the data is described by order_id, quantity, item_name, choice_descrptiption and price. Let’s try to look at which item is most ordered by people on the list. You can use matplotlib.pyplot to visualize the data.

import pandas as pd
df = pd.read_table('orders.tsv')
print(df.head(10))

import matplotlib.pyplot as plt
items = df.item_name.value_counts().plot(kind=&quot;bar&quot;)
plt.show()




It seems like the most popular item is the “Chicken Bowl” and the least ordered items are the “Carnitas Salad” and the “Veggie Crispy Tacos”. Besides the “Chicken Bowl”, I am curious as to what other items are most ordered. Let’s zoom in on the graph a little bit closer and see what those are. To do this, type the following command.

top10 = df.item_name.value_counts()[:10].plot(kind=&quot;bar&quot;)
plt.show()




Now you can see the name of the items more clearly than before. If you look at the graph, it is interesting to see that two items (Canned Soda, bottled water) from the top 10 are beverages. Let’s look at what kind of canned soda people ordered the most.  It is interesting to see that Mountain Dew and Dr.Pepper ranked second and third respectively.

df['item_price'] = df['item_price'].str.replace('$','')  
df['item_price'] = df['item_price'].astype(float)
orders = df.groupby('order_id').sum()  
print(orders.head())




It was intriguing to explore the items that are most ordered by customers at Chipolte. Now, let’s try to look at the data from a business perspective and explore the data of price per order. First, you have to replace “$” with “” and change the data type to “float” in “item_price column”, because we would like to analyzes the numbers, instead of strings.

descriptions = df.groupby([&quot;item_name&quot;, &quot;choice_description&quot;])[&quot;order_id&quot;].count().reset_index(name=&quot;count&quot;)
descriptions = descriptions[descriptions['item_name'].str.contains(&quot;Canned Soda&quot;)]  
descriptions.sort_values('count',ascending=False)
print(descriptions)

descriptions.choice_description.value_counts().plot(kind=&quot;bar&quot;)
plt.show()


On average, it looks like people tend to spend 18.81 dollars per order. The minimum is 10.08 dollars per order and the maximum is 205 dollars per order. It is quite incredible to see that there are people who order 205 dollars worth of chipotle at a time. Maybe that person is catering a party or some other sort or large gathering.



It was fun exploring the data and finding a trend. I am planning to post more like this in the future. If you have any questions, feedback, advice or corrections please get in touch with me on Linkedin or email me at sunghokim@wustl.edu. I referred to this blog to write this post.

</description>
        <pubDate>Sat, 05 Aug 2017 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2017/08/05/chipotle/</link>
        <guid isPermaLink="true">http://localhost:4000/2017/08/05/chipotle/</guid>
        
        
      </item>
    
  </channel>
</rss>
