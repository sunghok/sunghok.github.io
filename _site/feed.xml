<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>SUNGHO KIM</title>
    <description>Data Science Portfolio</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Mon, 11 Sep 2017 13:07:50 -0400</pubDate>
    <lastBuildDate>Mon, 11 Sep 2017 13:07:50 -0400</lastBuildDate>
    <generator>Jekyll v3.5.2</generator>
    
      <item>
        <title>Exploring Chipotle Order Data</title>
        <description>I found a data is based on about 3,000 meals in about 1,800 Grubhub orders from July to December 2012. These are from two Chipotle restaurants in Wahsington D.C &amp;amp; East Lansing. It seemed interesting for me to explore dataset by asking questions like what items people order the most, what is the average order price and etc.

First, Load tsv file first by using Pandas and print(df.head(10)) to see the format of data and first 10 items on the list. I recommend you to do this on every data set, before you execute any commands on data.

import pandas as pd
df = pd.read_table('orders.tsv')
print(df.head(10))




As you can see it from table, the data is described by order_id, quantity, item_name, choice_descrptiption and price. Let’s try to look at which i is most popular on the list. You can use matplotlib.pyplot to visualize the data.

import matplotlib.pyplot as plt
items = df.item_name.value_counts().plot(kind=&quot;bar&quot;)
plt.show()


It seems like the most popular item is “Chicken Bowl” and unpopular items are “Carnitas Salad” and “Veggie Crispy Tacos”. Let’s zoom the graph little bit closer and see what those 10 items are. To do this, type the following command.

top10 = df.item_name.value_counts()[:10].plot(kind=&quot;bar&quot;)
plt.show()


Now you can see the name of items clearly than before. If you look at the top10 list, it is interesting to see that two items (Canned Soda, bottled water) from top10 are beverages. Let’s look at what kind of canned soda people ordered the most.

df['item_price'] = df['item_price'].str.replace('$','')  
df['item_price'] = df['item_price'].astype(float)
orders = df.groupby('order_id').sum()  
print(orders.head())


It was interesting to explore the items that are most ordered by cusomters from Chipolte. Now Let’s try to look at the data from business perspective and explore the data of price per order. First, you have to replace “$” with “” and change the data type to “float” in “item_price column” , because we would like to analyzes the numbers, instead of strings.

descriptions = df.groupby([&quot;item_name&quot;, &quot;choice_description&quot;])[&quot;order_id&quot;].count().reset_index(name=&quot;count&quot;)
descriptions = descriptions[descriptions['item_name'].str.contains(&quot;Canned Soda&quot;)]  
descriptions.sort_values('count',ascending=False)
print(descriptions)

descriptions.choice_description.value_counts().plot(kind=&quot;bar&quot;)
plt.show()


On average, it looks like people tend to spend 18.81 dollars per order. The minimum is 10.08 dollars per order and the maximum is 205 dollars per order. It is interesting to see that there is a person who orders 205 dollars worth of chipotle at one time. It looks like that person ordered catering or something.

It was fun to explore the data and find a trend in them. I am planning to post more like this in the future. If you have any questions, feedback, advice or corrections please get in touch with me on Linkedin or email me at sunghokim@wustl.edu I referred to this blog http://www.danielforsyth.me/pandas-burritos-analyzing-chipotle-order-data-2/ to write this post.

</description>
        <pubDate>Sun, 20 Aug 2017 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2017/08/20/chipotle/</link>
        <guid isPermaLink="true">http://localhost:4000/2017/08/20/chipotle/</guid>
        
        
      </item>
    
      <item>
        <title>Clustering NBA Players</title>
        <description>In this post, we’ll be using the K-nearest neighbors algorithm and euclidean distance to figure out figure out which NBA players are the most similar to Lebron James. I foudn out that Kemba Walker is the most simlar player to Lebron James according to the NBA data from 16-17 season.


import pandas as pd
import math

nba = pd.read_csv(&amp;quot;nba17.csv&amp;quot;)

# Lebron James

# Select only the numeric columns from the NBA dataset
nba_numeric = nba[distance_columns]
# Normalize all of the numeric columns
nba_normalized = (nba_numeric - nba_numeric.mean()) / nba_numeric.std()




from scipy.spatial import distance
# Fill in NA values in nba_normalized
nba_normalized.fillna(0, inplace=True)
# Find the normalized vector for lebron james.
lebron_normalized = nba_normalized[nba[&amp;quot;player&amp;quot;] == &amp;quot;Lebron James&amp;quot;]


# Find the distance between lebron james and everyone else.
euclidean_distances = nba_normalized.apply(lambda row: distance.euclidean(row, lebron_normalized), axis=1)


# Create a new dataframe with distances.
distance_frame = pd.DataFrame(data={&amp;quot;dist&amp;quot;: euclidean_distances, &amp;quot;idx&amp;quot;: euclidean_distances.index})
distance_frame = distance_frame.sort_values([&amp;quot;dist&amp;quot;], ascending=True)

print(distance_frame)


# Find the most similar player to lebron (the lowest distance to lebron is lebron, the second smallest is the most similar non-lebron player)
second_smallest = distance_frame.iloc[1][&amp;quot;idx&amp;quot;]
most_similar_to_lebron = nba.loc[int(second_smallest)][&amp;quot;player&amp;quot;]
print(most_similar_to_lebron )

</description>
        <pubDate>Tue, 15 Aug 2017 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2017/08/15/nba_knn/</link>
        <guid isPermaLink="true">http://localhost:4000/2017/08/15/nba_knn/</guid>
        
        
      </item>
    
      <item>
        <title>Mapping NYC Motor Vehicle Collisions Data</title>
        <description>Everyday when I walk around NYC, I see a lot of traffics and sometimes car accidents on the street. This made me wonder if there are certain spots where the car accidents happen more frequently than others. If there are patterns, I thought it would be interesting to visualize and show them on the map. To solve my curiosity, I organized the procedures as following.


  Find the dataset
  Clean the dataset for mapping
  Visualize the dataset on Google map



 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33import gmplot 
import datetime
import pandas as pd
import matplotlib.pyplot as plt  


gmap = gmplot.GoogleMapPlotter(40.712784,-74.005941, 12)
df = pd.read_csv(&amp;quot;2017.csv&amp;quot;)
df = df.dropna(axis=0)

df[&amp;quot;LATITUDE&amp;quot;] = df[&amp;quot;LATITUDE&amp;quot;].astype(float)
df[&amp;quot;LONGITUDE&amp;quot;] = df[&amp;quot;LONGITUDE&amp;quot;].astype(float)
df = df[[&amp;quot;LATITUDE&amp;quot;,&amp;quot;LONGITUDE&amp;quot;]]
df.columns =[&amp;quot;lat&amp;quot;,&amp;quot;long&amp;quot;]


df[&amp;quot;coordinate&amp;quot;] = list(zip(df.lat, df.long))
df[&amp;#39;freq&amp;#39;] = df.groupby(&amp;#39;coordinate&amp;#39;)[&amp;#39;coordinate&amp;#39;].transform(&amp;#39;count&amp;#39;)

mapping = df[[&amp;quot;lat&amp;quot;,&amp;quot;long&amp;quot;,&amp;quot;freq&amp;quot;]]


yellow = mapping[mapping[&amp;quot;freq&amp;quot;]&amp;lt;=3] 
green  = mapping[(mapping[&amp;quot;freq&amp;quot;]&amp;gt;3) &amp;amp; (mapping[&amp;quot;freq&amp;quot;]&amp;lt;10)]
orange = mapping[(mapping[&amp;quot;freq&amp;quot;]&amp;lt;18) &amp;amp; (mapping[&amp;quot;freq&amp;quot;]&amp;gt;=10)]
red    = mapping[mapping[&amp;quot;freq&amp;quot;]&amp;gt;17] 


gmap.scatter(yellow.lat,yellow.long, &amp;quot;yellow&amp;quot; , size=15, marker=False)  
gmap.scatter(green.lat,green.long, &amp;quot;green&amp;quot; , size=20, marker=False)  
gmap.scatter(orange.lat,orange.long, &amp;quot;orange&amp;quot; , size=25, marker=False)  
gmap.scatter(red.lat,red.long, &amp;quot;red&amp;quot; , size=25, marker=False)  
gmap.draw(&amp;quot;colorful2.html&amp;quot;)


Finding the dataset
As always, choosing the right dataset is hardest problem in data analysis process. However, I was able to find the dataset called “NYPD Motor Vehicle Collisions” from NYC Open Data website. It provides open data for people to use and explore for free. It not only provides city government or transportation data, but also education, business and more. So if you are interested what is happening around the town, I highly recommend you to take a look at the data and explore.

Cleaning the dataset 
The dataset has more than 10 columns and one million rows. If I mapped all the million locations on the map, the map would look like a Yayoi Kusama’s art work rather than a map. So I decided to reduce my dataset and focus on accidents that happened in recent 3 months.

First, drop all the rows that have “NA” Second, select the columns that are needed for mapping (“LATIUDE” &amp;amp; “LONGTITUDE”) Third, make new column called “coordinate” that represents the spot. Finally, make another column called “frequency” which represents the frequency of coordinate.

</description>
        <pubDate>Sat, 05 Aug 2017 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2017/08/05/nyc_taxi/</link>
        <guid isPermaLink="true">http://localhost:4000/2017/08/05/nyc_taxi/</guid>
        
        
      </item>
    
  </channel>
</rss>
